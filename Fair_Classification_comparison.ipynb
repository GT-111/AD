{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.config_utils import get_config\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set_with_certain_group_excluded(sensitive_variable_name, sensitive_variable_map, original_dataset):\n",
    "    # return a dictionary of dataframes, each dataframe is a dataset with certain group excluded\n",
    "    # the key is the excluded group\n",
    "    result_dic = {}\n",
    "    for key, value in sensitive_variable_map.items():\n",
    "        train_set = original_dataset[original_dataset[sensitive_variable_name] == value] \n",
    "        #get not equal to value\n",
    "        test_set = original_dataset[original_dataset[sensitive_variable_name] != value]\n",
    "        result_dic[key] = {\n",
    "            'train_set': train_set,\n",
    "            'test_set': test_set\n",
    "        }\n",
    "    return result_dic\n",
    "\n",
    "\n",
    "def get_data_set_original(dataset, config):\n",
    "    warnings.warn(\"Data leakage problem\", DeprecationWarning)\n",
    "    train_set, test_set = train_test_split(dataset, test_size=config['test_size'], random_state=config['random_state'])\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def split_dataset_by_map(sensitive_variable_name, sensitive_variable_map, original_dataset, test_size = 0.4):\n",
    "    warnings.warn(\"Data leakage problem\", DeprecationWarning)\n",
    "    result_dic = {}\n",
    "    for key, value in sensitive_variable_map.items():\n",
    "        dataset = original_dataset[original_dataset[sensitive_variable_name] == value] \n",
    "        train_set, test_set = train_test_split(dataset, test_size=test_size)\n",
    "        result_dic[key] = {\n",
    "            'train_set': train_set,\n",
    "            'test_set': test_set\n",
    "        }\n",
    "    return result_dic\n",
    "\n",
    "\n",
    "def get_model(name, params):\n",
    "\n",
    "    if name == 'lr':\n",
    "        model = LogisticRegression(**params)\n",
    "    elif name == 'svm':\n",
    "        model = SVC(**params)\n",
    "    elif name == 'rf':\n",
    "        model = RandomForestClassifier(**params)\n",
    "    elif name == 'gb':\n",
    "        model = GradientBoostingClassifier(**params)\n",
    "    elif name == 'nn':\n",
    "        model = MLPClassifier(**params)\n",
    "    else:\n",
    "        raise ValueError('No such model')\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_data(features, config):\n",
    "    \n",
    "    features = features.copy()\n",
    "    drop_columns = config['drop_columns']\n",
    "    # drop the columns\n",
    "    features = features.drop(drop_columns, axis=1)\n",
    "    # get the missing values threshold\n",
    "    missing_values_threshold = config['missing_values_threshold']\n",
    "    #calculate missing values ratio\n",
    "    missing_ratio = features.isnull().mean()\n",
    "    # print(f'missing_ratio: {missing_ratio}')\n",
    "    #retain the values which are less than the threshold\n",
    "    variables_to_be_retained = features.columns[missing_ratio <= missing_values_threshold]\n",
    "    features = features[variables_to_be_retained]\n",
    "    # The samplest way to fill missing numerical values is to fill them with 0\n",
    "    features = features.fillna(0)\n",
    "    # check if there are any null values\n",
    "    has_null_values = features.isnull().any().any()\n",
    "    # if has_null_values:\n",
    "    #     print(\"has null values.\")\n",
    "    # else:\n",
    "    #     print(\"has no null values.\")\n",
    "\n",
    "    string_columns = features.select_dtypes(include='object').columns\n",
    "    # print the variables that are numerical(not strings)\n",
    "    num_string_columns = len(string_columns)\n",
    "    # print(f\"String Columns in Dataframe are are listed below. There are {num_string_columns} colums in total.\")\n",
    "\n",
    "    # for col in string_columns:\n",
    "    #     print(col)\n",
    "    # drop the string columns\n",
    "    features = features.drop(string_columns, axis=1)\n",
    "    # One-Hot Encoding\n",
    "    features = pd.get_dummies(features)\n",
    "    # normalize\n",
    "    normalize_method = config['normalize_method']\n",
    "    if normalize_method == 'min_max':\n",
    "        features = (features - features.min()) / (features.max() - features.min())\n",
    "    elif normalize_method == 'z_score':\n",
    "        features = (features - features.mean()) / features.std()\n",
    "    else:\n",
    "        raise ValueError('No such normalize method')\n",
    "    # There maybe some identical columns in the dataset, so we need to remove them. (Maybe there are other ways to do this)\n",
    "    columns_with_null = features.columns[features.isnull().any()]\n",
    "\n",
    "    # Print the column names with null values\n",
    "    # print(\"Columns with null values:\")\n",
    "    # print(columns_with_null)\n",
    "    features.drop(columns_with_null, axis=1, inplace=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred_prob):\n",
    "    # Threshold 0,5 by default\n",
    "    y_pred = np.where(y_pred_prob >= 0.5, 1, 0)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Recall\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "\n",
    "    # Specificity\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # ROC AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # FPR FNR\n",
    "    false_positive_rate = fpr[1]\n",
    "    false_negative_rate = 1 - tpr[1]\n",
    "\n",
    "    # confusion_matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    result_dic = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensitivity (Recall)\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"False Positive Rate\": false_positive_rate,\n",
    "        \"False Negative Rate\": false_negative_rate,\n",
    "    }\n",
    "    \n",
    "    return result_dic, (fpr, tpr), conf_matrix\n",
    "\n",
    "\n",
    "def plot_auc(metrics, tag, save_fig = True):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model_name, metric in metrics.items():\n",
    "        fpr, tpr = metric['fpr_tpr']\n",
    "        roc_auc = metric['ROC AUC']\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic {tag}')\n",
    "    plt.legend(loc='lower right')\n",
    "    path = os.path.join(\"pics\", f'roc_curve{tag}.png')\n",
    "    if save_fig :\n",
    "        plt.savefig(path)\n",
    "\n",
    "\n",
    "def stratified_group_kfold(df, config):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling similar to StratifiedGroupKFold, while ensuring test set size is close to the specified test_size ratio.\n",
    "    \n",
    "    Returns:\n",
    "    train_indices, test_indices: Lists of training and testing indices.\n",
    "    \"\"\"\n",
    "    target_col = config['target_variable']['name']\n",
    "\n",
    "    # Get the group variable for stratified sampling\n",
    "    group_col = config['groups']\n",
    "    # Get the random seed\n",
    "    random_state = config['random_state']\n",
    "    # Get the k_fold\n",
    "    k_fold = config['k_fold']\n",
    "    # Get the test_size\n",
    "    test_size = config['test_size']\n",
    "    X = df.drop(columns=[target_col, group_col]).values\n",
    "    y = df[target_col].values\n",
    "    groups = df[group_col].values\n",
    "    \n",
    "    stratified_kfold = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=random_state)\n",
    "    train_indices, test_indices = [], []\n",
    "    \n",
    "    for train_idx, test_idx in stratified_kfold.split(X, y):\n",
    "        X_train, X_test, y_train, y_test, groups_train, groups_test = \\\n",
    "            X[train_idx], X[test_idx], y[train_idx], y[test_idx], groups[train_idx], groups[test_idx]\n",
    "        \n",
    "        # Calculate the target size for the test set\n",
    "        target_test_size = int(len(X) * test_size)\n",
    "        \n",
    "        # If the test set size is less than the target size, move additional samples from the train set to the test set\n",
    "        if len(test_idx) < target_test_size:\n",
    "            additional_samples = target_test_size - len(test_idx)\n",
    "            rng = np.random.default_rng(random_state)\n",
    "            additional_samples_indices = rng.choice(len(train_idx), additional_samples, replace=False)\n",
    "            \n",
    "            test_idx = np.concatenate((test_idx, train_idx[additional_samples_indices]))\n",
    "            train_idx = np.delete(train_idx, additional_samples_indices)\n",
    "        \n",
    "        train_indices.append(train_idx)\n",
    "        test_indices.append(test_idx)\n",
    "    X = df.drop(columns=[target_col, group_col]).copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    return train_indices, test_indices, X, y\n",
    "\n",
    "\n",
    "def split_dataset_by_sensitive_variable(protected_attribute_name, protected_attribute_map, original_dataset, config):\n",
    "    #split the dataset by sensitive variable\n",
    "    # \n",
    "    original_dataset = original_dataset.copy()\n",
    "    target_col = config['target_variable']['name']\n",
    "    # Get the group variable for stratified sampling\n",
    "    group_col = config['groups']\n",
    "    # Get the random seed\n",
    "    random_state = config['random_state']\n",
    "    # Get the k_fold\n",
    "    k_fold = config['k_fold']\n",
    "    # Get the test_size\n",
    "    test_size = config['test_size']\n",
    "\n",
    "    X = original_dataset.drop(columns=[target_col, group_col, protected_attribute_name])\n",
    "    X = preprocess_data(X, config['preprocessing'])\n",
    "    y = original_dataset[target_col]\n",
    "    protected_attribute = original_dataset[protected_attribute_name]\n",
    "    groups = original_dataset[group_col]\n",
    "    original_dataset = pd.concat([X, y, protected_attribute, groups], axis=1)\n",
    "    result_dic = {}\n",
    "\n",
    "    for key, value in protected_attribute_map.items():\n",
    "\n",
    "        dataset = original_dataset[original_dataset[protected_attribute_name] == value].copy()\n",
    "        train_indices, test_indices, X, y = stratified_group_kfold(df = dataset, config=config)\n",
    "        result_dic[key] = {\n",
    "            'train_set_indices': train_indices,\n",
    "            'test_set_indices': test_indices,\n",
    "            'X': X,\n",
    "            'y': y\n",
    "        }\n",
    "    \n",
    "    return result_dic\n",
    "\n",
    "\n",
    "def get_per_sensitive_variable_split(protected_attributes, original_dataset, config):\n",
    "    datasets_dic = {}\n",
    "    for protected_attribute in protected_attributes.values():\n",
    "\n",
    "        protected_attribute_name = protected_attribute['name']\n",
    "        protected_attribute_map = protected_attribute['map']\n",
    "        datasets_dic[protected_attribute_name] = split_dataset_by_sensitive_variable(protected_attribute_name = protected_attribute_name,\n",
    "                                                                                            protected_attribute_map=protected_attribute_map, \n",
    "                                                                                            original_dataset = original_dataset,\n",
    "                                                                                            config=config)\n",
    "    return datasets_dic\n",
    "\n",
    "\n",
    "def train_and_evaluate_one_model(model, dataset_dic, config):\n",
    "    target_variable = config['target_variable']['name']\n",
    "    target_variable_map = config['target_variable']['map']\n",
    "    #metrics dic for all classes\n",
    "    result_dic = {}\n",
    "    k_fold = config['k_fold']\n",
    "    \n",
    "    for c_train, dic_train in dataset_dic.items():\n",
    "        \n",
    "        result_dic[c_train] = {}\n",
    "        # metrics dic for a single class\n",
    "        for fold in range(k_fold):\n",
    "            X = dic_train['X'].copy()\n",
    "            y = dic_train['y'].copy()\n",
    "            y = y.map(target_variable_map)\n",
    "            print(f\"Training and Testing on fold {fold} for class {c_train}\")\n",
    "            # Get the train set indice for the current fold\n",
    "            train_set_indice = dic_train['train_set_indices'][fold]\n",
    "            X_train, y_train = X.iloc[train_set_indice], y.iloc[train_set_indice]\n",
    "            model.fit(X_train, y_train)\n",
    "            for c_test, dic_test in dataset_dic.items():\n",
    "                # print(f\"Testing on class {c_test}\")\n",
    "                X = dic_test['X'].copy()\n",
    "                y = dic_test['y'].copy()\n",
    "                y = y.map(target_variable_map)\n",
    "                # Get the test set indice for the current fold\n",
    "                test_set_indice = dic_test['test_set_indices'][fold]\n",
    "                X_test, y_test = X.iloc[test_set_indice], y.iloc[test_set_indice]\n",
    "                y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "                #calculate metrics\n",
    "                metrics, _, _ = calculate_classification_metrics(y_true=y_test, y_pred_prob=y_pred_prob)\n",
    "                # acc = metrics['Accuracy']\n",
    "                # print(f'{c_train} vs {c_test}: {acc}')\n",
    "                if  not c_test in result_dic[c_train]:\n",
    "                    result_dic[c_train][c_test] = metrics\n",
    "                else :\n",
    "                    for key, value in metrics.items():\n",
    "                        result_dic[c_train][c_test][key] += value\n",
    "                        \n",
    "        # Get the average of the metrics\n",
    "        for c_test, dic in dataset_dic.items():\n",
    "            for key, value in result_dic[c_train][c_test].items():\n",
    "                result_dic[c_train][c_test][key] = value / k_fold\n",
    "\n",
    "    return result_dic\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(models_dict, datasets_dic, config):\n",
    "    \n",
    "    result_dic = {}\n",
    "\n",
    "    for model_name, model_params in models_dict.items():\n",
    "\n",
    "        result_dic[model_name] = {}\n",
    "        \n",
    "        for protected_attribute_name, dataset_dic in datasets_dic.items():\n",
    "\n",
    "            result_dic[model_name][protected_attribute_name] = {}\n",
    "            model = get_model(model_name, model_params)\n",
    "            result_dic[model_name][protected_attribute_name] = train_and_evaluate_one_model(model=model, dataset_dic=dataset_dic, config=config)\n",
    "        \n",
    "    return result_dic\n",
    "\n",
    "\n",
    "def result_dic2csv(result_dic, metric_name):\n",
    "    metrics = ['Accuracy', 'Sensitivity (Recall)', 'Specificity', 'F1 Score', 'ROC AUC', 'False Positive Rate', 'False Negative Rate']\n",
    "    if metric_name not in metrics:\n",
    "        raise ValueError('No such metric')\n",
    "    result_dic = result_dic.copy()\n",
    "    \n",
    "    for protected_attribute, protected_attribute_dic in result_dic.items():\n",
    "        \n",
    "        metrics = {}\n",
    "        for c_1, dic in protected_attribute_dic.items():\n",
    "            \n",
    "            metrics[c_1] = {}\n",
    "            \n",
    "            for c_2, metrics_ in dic.items():\n",
    "\n",
    "                metrics[c_1][c_2] = metrics_[metric_name]\n",
    "\n",
    "        df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "        df.to_csv(f'{protected_attribute}_{metric_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on one group and evaluate on all the groups\n",
    "# e.g. train on trainning dataset of White people and evaluate on White, black, Asian, etc.\n",
    "\n",
    "# The path of NACC Dataset\n",
    "NACC_DATASET_PATH = os.path.join(\"data\", \"NACC.csv\")\n",
    "# The path of the Config file \n",
    "DATA_CONFIG_PATH = os.path.join(\"config\", \"config_default.yaml\")\n",
    "# Load the config file\n",
    "config = get_config(DATA_CONFIG_PATH)\n",
    "# Load the NACC dataset\n",
    "NACC_dataset = pd.read_csv(NACC_DATASET_PATH, low_memory=False)\n",
    "# Get the model dictionary\n",
    "models_dict = config['model_dict']\n",
    "# Get the protected attributes\n",
    "protected_attributes = config['protected_attributes']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing on fold 0 for class White\n",
      "Training and Testing on fold 1 for class White\n",
      "Training and Testing on fold 2 for class White\n",
      "Training and Testing on fold 3 for class White\n",
      "Training and Testing on fold 0 for class black_or_African_American\n",
      "Training and Testing on fold 1 for class black_or_African_American\n",
      "Training and Testing on fold 2 for class black_or_African_American\n",
      "Training and Testing on fold 3 for class black_or_African_American\n",
      "Training and Testing on fold 0 for class American_Indian_or_Alaska_Native\n",
      "Training and Testing on fold 1 for class American_Indian_or_Alaska_Native\n",
      "Training and Testing on fold 2 for class American_Indian_or_Alaska_Native\n",
      "Training and Testing on fold 3 for class American_Indian_or_Alaska_Native\n",
      "Training and Testing on fold 0 for class Asian\n",
      "Training and Testing on fold 1 for class Asian\n",
      "Training and Testing on fold 2 for class Asian\n",
      "Training and Testing on fold 3 for class Asian\n",
      "Training and Testing on fold 0 for class Male\n",
      "Training and Testing on fold 1 for class Male\n",
      "Training and Testing on fold 2 for class Male\n",
      "Training and Testing on fold 3 for class Male\n",
      "Training and Testing on fold 0 for class Female\n",
      "Training and Testing on fold 1 for class Female\n",
      "Training and Testing on fold 2 for class Female\n",
      "Training and Testing on fold 3 for class Female\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset by sensitive variable\n",
    "datasets_dic = get_per_sensitive_variable_split(protected_attributes = protected_attributes,\n",
    "                                                original_dataset=NACC_dataset.copy(),\n",
    "                                                config=config)\n",
    "# Train and evaluate the models\n",
    "result_dic = train_and_evaluate_models(models_dict, datasets_dic, config)\n",
    "# Save the result to csv\n",
    "result_dic2csv(result_dic, 'ROC AUC')\n",
    "result_dic2csv(result_dic, 'Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
