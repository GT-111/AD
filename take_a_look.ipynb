{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the nacc dataset and config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NACC.csv\n",
      "config/data_config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# The path of NACC Dataset\n",
    "NACC_DATASET_PATH = os.path.join(\"data\", \"NACC.csv\")\n",
    "# The path of the Config file \n",
    "DATA_CONFIG_PATH = os.path.join(\"config\", \"data_config.yaml\")\n",
    "print(NACC_DATASET_PATH)\n",
    "print(DATA_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'num_epochs': 100, 'target_variable': ['NACCALZD'], 'missing_values_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.config_utils import get_config\n",
    "\n",
    "config = get_config(DATA_CONFIG_PATH)\n",
    "print(config)\n",
    "\n",
    "df = pd.read_csv(NACC_DATASET_PATH, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25368, 996)\n",
      "(25368, 835)\n"
     ]
    }
   ],
   "source": [
    "target_variable = list(config['target_variable'])\n",
    "missing_values_threshold = config['missing_values_threshold']\n",
    "\n",
    "Y = df[target_variable].copy()\n",
    "X = df.drop(columns=target_variable, axis=1)\n",
    "print(X.shape)\n",
    "#calculate missing values ratio\n",
    "missing_ratio = X.isnull().mean()\n",
    "#retain the values which are less than the threshold\n",
    "variables_to_be_retained = X.columns[missing_ratio <= missing_values_threshold]\n",
    "X_filtered = X[variables_to_be_retained]\n",
    "print(X_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has no null values.\n"
     ]
    }
   ],
   "source": [
    "# # For being simple, text are converted to 1, emopty cols are converted to 0\n",
    "# X_filled = X_filtered.applymap(lambda x: 1 if isinstance(x, str) and x.strip() != '' else x)\n",
    "\n",
    "# The samplest way to fill missing numerical values is to fill them with 0\n",
    "X_filled = X_filtered.fillna(0)\n",
    "# check if there are any null values\n",
    "has_null_values = X_filled.isnull().any().any()\n",
    "if has_null_values:\n",
    "    print(\"has null values.\")\n",
    "else:\n",
    "    print(\"has no null values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Columns in Dataframe are are listed below. There are 14 colums in total.\n",
      "NACCID\n",
      "PACKET\n",
      "DRUG1\n",
      "DRUG2\n",
      "DRUG3\n",
      "DRUG4\n",
      "DRUG5\n",
      "DRUG6\n",
      "DRUG_ID1\n",
      "DRUG_ID2\n",
      "DRUG_ID3\n",
      "DRUG_ID4\n",
      "DRUG_ID5\n",
      "DRUG_ID6\n"
     ]
    }
   ],
   "source": [
    "# print the variables that are strings(not numerical)\n",
    "string_columns = X_filled.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "# print the variables that are numerical(not strings)\n",
    "num_string_columns = len(string_columns)\n",
    "print(f\"String Columns in Dataframe are are listed below. There are {num_string_columns} colums in total.\")\n",
    "\n",
    "for col in string_columns:\n",
    "    print(col)\n",
    "\"\"\"\n",
    "    TODO: discuss how to deal with the string columns\n",
    "    1. drop the columns (current approach)\n",
    "\"\"\" \n",
    "# drop the string columns\n",
    "X_filled = X_filled.drop(string_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "X_encoded = pd.get_dummies(X_filled)\n",
    "# 0-8 - Diagnosis of Alzheimer's Disease (NACCALZD)\n",
    "Y['NACCALZD'] = Y['NACCALZD'].map({0: 'Undiagnosed', 1: 'Diagnosed', 8: 'Undiagnosed'})\n",
    "\n",
    "Y_encoded = pd.get_dummies(Y, columns=['NACCALZD'])\n",
    "Y_encoded = Y_encoded.astype(float)\n",
    "# Normalization\n",
    "# 1. Min-Max Normalization\n",
    "X_min_max = (X_filled - X_filled.min(numeric_only=True)) / (X_filled.max(numeric_only=True) - X_filled.min(numeric_only=True))\n",
    "\n",
    "\n",
    "# 2. Z-Score Normalization\n",
    "X_z_score = (X_filled - X_filled.mean(numeric_only=True)) / X_filled.std(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with null values:\n",
      "Index(['DOWNS', 'HUNT', 'NACCMRSA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# There maybe some identical columns in the dataset, so we need to remove them. (Maybe there are other ways to do this)\n",
    "columns_with_null = X_z_score.columns[X_z_score.isnull().any()]\n",
    "\n",
    "# Print the column names with null values\n",
    "print(\"Columns with null values:\")\n",
    "print(columns_with_null)\n",
    "X_z_score.drop(columns_with_null, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = X_z_score\n",
    "# features = X_min_max\n",
    "labels = Y_encoded\n",
    "COL_NUM = features.shape[1]\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define a custom PyTorch Dataset for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create CustomDataset instances for training and testing sets\n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "batch_size = config['batch_size']\n",
    "# batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [10:43<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "num_epochs = config['num_epochs']\n",
    "# num_epochs = 10\n",
    "\n",
    "\n",
    "# Define a simple MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):    \n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.bacth_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self._init()\n",
    "    def _init(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0.1)\n",
    "        nn.init.constant_(self.fc2.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bacth_norm(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create the MLP model\n",
    "CLASS_NUM = 2\n",
    "mlp_model = MLP(COL_NUM, 640, CLASS_NUM)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp_model.parameters(), lr=0.01)\n",
    "for epoch in tqdm.trange(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(inputs)\n",
    "        loss = criterion(outputs, targets)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[1704    0]\n",
      " [   0 3370]]\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "F1 Score: 1.0\n",
      "AUC: 1.0\n",
      "False Positive Rate: 0.0\n",
      "False Negative Rate: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1704\n",
      "           1       1.00      1.00      1.00      3370\n",
      "\n",
      "    accuracy                           1.00      5074\n",
      "   macro avg       1.00      1.00      1.00      5074\n",
      "weighted avg       1.00      1.00      1.00      5074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mlp_model.eval()\n",
    "    y_pred_tensor = mlp_model(X_test_tensor)\n",
    "    y_pred = torch.argmax(y_pred_tensor, 1)\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_test = torch.argmax(y_test_tensor, 1)\n",
    "    y_test = y_test.numpy()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n",
    "# ROC & AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC:\", roc_auc)\n",
    "\n",
    "# False Positive Rate & False Negative Rate\n",
    "false_positive_rate = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])\n",
    "false_negative_rate = conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "print(\"False Positive Rate:\", false_positive_rate)\n",
    "print(\"False Negative Rate:\", false_negative_rate)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
