{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the nacc dataset and config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The path of NACC Dataset\n",
    "NACC_DATASET_PATH = os.path.join(\"data\", \"NACC.csv\")\n",
    "# The path of the Config file \n",
    "DATA_CONFIG_PATH = os.path.join(\"config\", \"data_config.yaml\")\n",
    "print(NACC_DATASET_PATH)\n",
    "print(DATA_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.config_utils import get_config\n",
    "\n",
    "config = get_config(DATA_CONFIG_PATH)\n",
    "print(config)\n",
    "\n",
    "df = pd.read_csv(NACC_DATASET_PATH, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = list(config['target_variable'])\n",
    "missing_values_threshold = config['missing_values_threshold']\n",
    "\n",
    "Y = df[target_variable].copy()\n",
    "X = df.drop(columns=target_variable, axis=1)\n",
    "print(X.shape)\n",
    "#calculate missing values ratio\n",
    "missing_ratio = X.isnull().mean()\n",
    "#retain the values which are less than the threshold\n",
    "variables_to_be_retained = X.columns[missing_ratio <= missing_values_threshold]\n",
    "X_filtered = X[variables_to_be_retained]\n",
    "print(X_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For being simple, text are converted to 1, emopty cols are converted to 0\n",
    "# X_filled = X_filtered.applymap(lambda x: 1 if isinstance(x, str) and x.strip() != '' else x)\n",
    "\n",
    "# The samplest way to fill missing numerical values is to fill them with 0\n",
    "X_filled = X_filtered.fillna(0)\n",
    "# check if there are any null values\n",
    "has_null_values = X_filled.isnull().any().any()\n",
    "if has_null_values:\n",
    "    print(\"has null values.\")\n",
    "else:\n",
    "    print(\"has no null values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the variables that are strings(not numerical)\n",
    "groups = X_filled['NACCID']\n",
    "string_columns = X_filled.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "# print the variables that are numerical(not strings)\n",
    "num_string_columns = len(string_columns)\n",
    "print(f\"String Columns in Dataframe are are listed below. There are {num_string_columns} colums in total.\")\n",
    "\n",
    "for col in string_columns:\n",
    "    print(col)\n",
    "\"\"\"\n",
    "    TODO: discuss how to deal with the string columns\n",
    "    1. drop the columns (current approach)\n",
    "\"\"\" \n",
    "# drop the string columns\n",
    "X_filled = X_filled.drop(string_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "X_encoded = pd.get_dummies(X_filled)\n",
    "# 0-8 - Diagnosis of Alzheimer's Disease (NACCALZD)\n",
    "# Y['NACCALZD'] = Y['NACCALZD'].map({0: 'Undiagnosed', 1: 'Diagnosed', 8: 'Undiagnosed'})\n",
    "Y['NACCALZD'] = Y['NACCALZD'].map({0: 0, 1: 1, 8: 0})\n",
    "# Y_encoded = pd.get_dummies(Y, columns=['NACCALZD'])\n",
    "Y_encoded = Y.astype(float)\n",
    "# Normalization\n",
    "# 1. Min-Max Normalization\n",
    "X_min_max = (X_filled - X_filled.min(numeric_only=True)) / (X_filled.max(numeric_only=True) - X_filled.min(numeric_only=True))\n",
    "\n",
    "\n",
    "# 2. Z-Score Normalization\n",
    "X_z_score = (X_filled - X_filled.mean(numeric_only=True)) / X_filled.std(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There maybe some identical columns in the dataset, so we need to remove them. (Maybe there are other ways to do this)\n",
    "columns_with_null = X_z_score.columns[X_z_score.isnull().any()]\n",
    "\n",
    "# Print the column names with null values\n",
    "print(\"Columns with null values:\")\n",
    "print(columns_with_null)\n",
    "X_z_score.drop(columns_with_null, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "features = X_z_score.values\n",
    "# features = X_min_max\n",
    "labels = Y_encoded.values\n",
    "COL_NUM = features.shape[1]\n",
    "\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "gbt_model = GradientBoostingClassifier()\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 6],\n",
    "    'subsample': [0.6, 1.0]\n",
    "}\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=3)\n",
    "\n",
    "print('StratifiedGroupKFold:')\n",
    "for train_indices, test_indices in sgkf.split(features, labels, groups):\n",
    "    X_train, X_test = features[train_indices], features[test_indices]\n",
    "    y_train, y_test = labels[train_indices], labels[test_indices]\n",
    "    y_train, y_test = y_train.ravel(), y_test.ravel()\n",
    "    gbt_model.fit(X_train, y_train)\n",
    "    y_pred = gbt_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print('GridSearchCV + StratifiedGroupKFold:')\n",
    "for train_indices, test_indices in sgkf.split(features, labels, groups):\n",
    "    X_train, X_test = features[train_indices], features[test_indices]\n",
    "    y_train, y_test = labels[train_indices], labels[test_indices]\n",
    "    y_train, y_test = y_train.ravel(), y_test.ravel()\n",
    "\n",
    "    grid_search = GridSearchCV(gbt_model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # 输出最佳参数组合和对应的性能\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "    # 在测试集上评估最佳模型\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print('GridSearchCV:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "grid_search = GridSearchCV(gbt_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n",
    "# ROC & AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC:\", roc_auc)\n",
    "\n",
    "# False Positive Rate & False Negative Rate\n",
    "false_positive_rate = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])\n",
    "false_negative_rate = conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "print(\"False Positive Rate:\", false_positive_rate)\n",
    "print(\"False Negative Rate:\", false_negative_rate)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
